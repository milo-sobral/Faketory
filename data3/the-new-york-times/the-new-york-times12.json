{"id": "No, A.I. Won\u2019t Solve the Fake News Problem", "paragraph": "Supported byAt least not for decades to come. Sorry, Mark Zuckerberg.By Gary Marcus and Ernest DavisDr. Marcus is a professor of psychology and neural science. Dr. Davis is a professor of computer science.In his testimony before Congress this year, Mark Zuckerberg, the chief executive of Facebook, addressed concerns about the strategically disseminated misinformation known as fake news that may have affected the outcome of the 2016 presidential election. Have no fear, he assured Congress, a solution was on its way \u2014 if not next year, then at least \u201cover a five- to 10-year period.\u201dThe solution? Artificial intelligence. Mr. Zuckerberg\u2019s vision, which the committee members seemed to accept, was that soon enough, Facebook\u2019s A.I. programs would be able to detect fake news, distinguishing it from more reliable information on the platform.With midterms approaching, along with the worrisome prospect that fake news could once again influence our elections, we wish we could say we share Mr. Zuckerberg\u2019s optimism. But in the near term we don\u2019t find his vision plausible. Decades from now, it may be possible to automate the detection of fake news. But doing so would require a number of major advances in A.I., taking us far beyond what has so far been invented.As Mr. Zuckerberg has acknowledged, today\u2019s A.I. operates at the \u201ckeyword\u201d level, flagging word patterns and looking for statistical correlations among them and their sources. This can be somewhat useful: Statistically speaking, certain patterns of language may indeed be associated with dubious stories. For instance, for a long period, most articles that included the words \u201cBrad,\u201d \u201cAngelina\u201d and \u201cdivorce\u201d turned out to be unreliable tabloid fare. Likewise, certain sources may be associated with greater or lesser degrees of factual veracity. The same account deserves more credence if it appears in The Wall Street Journal than in The National Enquirer.But none of these kinds of correlations reliably sort the true from the false. In the end, Brad Pitt and Angelina Jolie did get divorced. Keyword associations that might help you one day can fool you the next.To get a handle on what automated fake-news detection would require, consider an article posted in May on the far-right website WorldNetDaily, or WND. The article reported that a decision to admit girls, gays and lesbians to the Boy Scouts had led to a requirement that condoms be available at its \u201cglobal gathering.\u201d A key passage consists of the following four sentences:The Boy Scouts have decided to accept people who identify as gay and lesbian among their ranks. And girls are welcome now, too, into the iconic organization, which has renamed itself Scouts BSA. So what\u2019s next? A mandate that condoms be made available to \u2018all participants\u2019 of its global gathering.Was this account true or false? Investigators at the fact-checking site Snopes determined that the report was \u201cmostly false.\u201d But determining how it went afoul is a subtle business beyond the dreams of even the best current A.I.First of all, there is no telltale set of phrases. \u201cBoy Scouts\u201d and \u201cgay and lesbian,\u201d for example, have appeared together in many true reports before. Then there is the source: WND, though notorious for promoting conspiracy theories, publishes and aggregates legitimate news as well. Finally, sentence by sentence, there are a lot of true facts in the passage: Condoms have indeed been available at the global gathering that scouts attend, and the Boy Scouts organization has indeed come to accept girls as well as gays and lesbians into its ranks.What makes the article \u201cmostly false\u201d is that it implies a causal connection that doesn\u2019t exist. It strongly suggests that the inclusion of gays and lesbians and girls led to the condom policy (\u201cSo what\u2019s next?\u201d). But in truth, the condom policy originated in 1992 (or even earlier) and so had nothing to do with the inclusion of gays, lesbians or girls, which happened over just the past few years.Causal relationships are where contemporary machine learning techniques start to stumble. In order to flag the WND article as deceptive, an A.I. program would have to understand the causal implication of \u201cwhat\u2019s next?,\u201d recognize that the account implies that the condom policy was changed recently and know to search for information that is not supplied about when the various policies were introduced.Understanding the significance of the passage would also require understanding multiple viewpoints. From the perspective of the international organization for scouts, making condoms available at a global gathering of 30,000 to 40,000 hormone-laden adolescents is a prudent public health measure. From the point of view of WND, the availability of condoms, like the admission of girls, gays and lesbians to the Boy Scouts, is a sign that a hallowed institution has been corrupted.We are not aware of any A.I. system or prototype that can sort among the various facts involved in those four sentences, let alone discern the relevant implicit attitudes.Most current A.I. systems that process language are oriented around a different set of problems. Translation programs, for example, are primarily interested in a problem of correspondence \u2014 which French phrase, say, is the best parallel of a given English phrase? But determining that someone is implying, by a kind of moral logic, that the Boy Scouts\u2019 policy of inclusion led to condoms being supplied to scouts isn\u2019t a simple matter of checking a claim against a database of facts.Existing A.I. systems that have been built to comprehend news accounts are extremely limited. Such a system might be able to look at the passage from the WND article and answer a question whose answer is given directly and explicitly in the story (e.g., \u201cDoes the Boy Scouts organization accept people who identify as gay and lesbian?\u201d). But such systems rarely go much further, lacking a robust mechanism for drawing inferences or a way of connecting to a body of broader knowledge. As Eduardo Ari\u00f1o de la Rubia, a data scientist at Facebook, told us, for now \u201cA.I. cannot fundamentally tell what\u2019s true or false \u2014 this is a skill much better suited to humans.\u201dTo get to where Mr. Zuckerberg wants to go will require the development of a fundamentally new A.I. paradigm, one in which the goal is not to detect statistical trends but to uncover ideas and the relations between them. Only then will such promises about A.I. become reality, rather than science fiction.Gary Marcus, a professor of psychology and neural science at New York University, and Ernest Davis, a professor of computer science there, are working on a book about how to build trustworthy A.I.Follow The New York Times Opinion section on Facebook, Twitter (@NYTopinion) and Instagram,Advertisement"}